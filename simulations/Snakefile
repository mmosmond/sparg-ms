# -------------- python modules ---------------

# ------------- paths ------------------

DATADIR = 'data/' #where to put outputs
PROGRAMDIR = 'programs/' #where to put programs

#  ----------- parameters -----------------

# required for SLiM simulations
Ls = [int(1e8)] #number of basepairs
RBPs = [1e-8] #per base pair recombination rate
LAMBDAs = [2.0] #mean offspring per parent when no competition
Ks = [2.0] #carrying capacity per unit area
Ws = [50] #width of habitat square
SIGMAcomps = [0.5] #SD of competition kernel
SIGMAmates = [0.5] #SD of mate choice kernel
SIGMAdisps = [0.25,0.5,0.75,1,1.25,1.5,1.75,2] #SD of dispersal kernal
selfings = [0] #fraction of offspring selfed
MAXTs = [20000] #number of gens to run sim for (should be at least 4*W**2*K) 
NREPS = 10 #number of reps
nreps = range(NREPS)

# required for processing trees
Nes = [5000] #Ne for recapitation, make roughly W**2 * K
Us = [1e-8] #per base pair mutation rate
ks = [50] #number of individuals to sample for tree-sequence (this is half the number of chromosomes)
numiters = [5] #number of mcmc iterations to do when inferring coalescence rates
thresholds = [0.5] #fraction of trees to drop when inferring coalescence relates
treeskips = [100] #use every TREESKIPth tree
Ms = [1000] #number of samples of each tree for importance sampling
tCutoffs = [100,1000,10000,None] #veil of ignorance: only use info in this time to estimate dispersal

## for ancestor location sims
#anc_reps = range(1) #number of replicates for retained ancestor sims
#retain_gens = range(10,1001,10) #generations (from present) to retain ancestors in
#retain_gens_str = 'c('+','.join(map(str,retain_gens))+')' #reformat in Eidos
#anc_SIGMAds = [0.5] #SD of mate choice and dispersal kernal (symmetric, no correlation) 
#anc_nloci = 100 #number of loci to use
##anc_which_sites = np.linspace(0, 1e8-1e3, anc_nloci, dtype=int) #bp at which to get trees for mles, note we subtract some off L bc inferred sequences sometimes a bit shorter (as they come from the VCF, which lacks info on total genome length)
#anc_times  = retain_gens #choose times to locate ancestors
#styles = ['MLE','BLUP'] #different methods to locate ancestors
#anc_tCutoffs = [10000] #time cutoffs for estimating dispersal
#anc_loctCutoffs = [10000] #time cutoffs for locating ancestors (note that we might want to use trees with a smaller tCutoff to estimate dispersal more accurately, then apply this dispersal rate to deeper trees)
#
## for 2 epoch sims
#SIGMAd1s = [0.25, 0.5] #[0.5,1] #dispersal in most recent epoch
#SIGMAd2s = [0.25, 0.5] #[0.5,1] #dispersal in most distant epoch
#T12s = [100] #gen in past when dispersal rate changes
#epoch_reps = range(10) #number of reps to get mcles
#epoch_mle_reps = range(1) #number of reps to get per locus mles
#epoch_bnds = ((1e-6,None), (1e-6,None), (-0.99,0.99), (1e-6,None), (1e-6,None), (-0.99,0.99), (1e-6,None)) #bounds on parameters being estimated
#epoch_Ks = [4.0] #might have to increase density for small sigmas, since this is how mate choice is determined 
#epoch_Nes = [10000] #make K * W^2
#epoch_tCutoffs = [int(1e3)]#,int(1e4)]

##################
#### PROGRAMS ####
##################

# ------------------- slim ------------------

SLiM = PROGRAMDIR + 'SLiM_build/slim' #command to run SLiM

rule get_slim:
  input:
  output:
    SLiM
  shell:
    '''
    wget https://github.com/MesserLab/SLiM/releases/download/v4.0.1/SLiM.zip -P {PROGRAMDIR}
    cd {PROGRAMDIR}
    unzip SLiM.zip
    rm SLiM.zip
    module load cmake/3.21.4
    module load gcc/8.3.0
    mkdir build
    cd build
    cmake ../SLiM
    make slim
    cd .. 
    mv build/ SLiM_build
    '''

# ---------------------- relate --------------

RELATEDIR = PROGRAMDIR + 'relate' #where relate lives
RELATE = PROGRAMDIR + 'relate/bin/Relate' #the executable

rule get_relate:
  input:
  output:
    RELATE
  shell:
    '''
    git clone https://github.com/MyersGroup/relate.git
    cd relate/build
    module load cmake/3.21.4 gcc/8.3.0 gsl/2.5
    cmake ..
    make
    cd -
    mv relate/ {PROGRAMDIR}
    ''' 

#################################
### ONE EPOCH DISPERSAL RATES ###
#################################

# ------- SLiM simulations -------
# <1h w/ 80 threads, 1 each

slim_trees = DATADIR + "sim_{L}L_{RBP}RBP_{LAMBDA}LAMBDA_{K}K_{W}W_{SIGMAcomp}SIGMAcomp_{SIGMAmate}SIGMAmate_{SIGMAdisp}SIGMAdisp_{selfing}selfing_{MAXT}MAXT_{nrep}nrep.trees"

#rule slim_sims:
#  input:
#    expand(slim_trees, L=Ls, RBP=RBPs, LAMBDA=LAMBDAs, K=Ks, W=Ws, SIGMAcomp=SIGMAcomps, SIGMAmate=SIGMAmates, SIGMAdisp=SIGMAdisps, selfing=selfings, MAXT=MAXTs, nrep=nreps)

rule slim_sim:
  input:
    SLiM,
    "scripts/sim.slim"
  output:
    slim_trees
  group: "slim_sims"	 
  threads: 1
  shell:
    """
    module load gcc/8.3.0    
    mkdir -p {DATADIR}
    {input[0]} \
      -d L={wildcards.L} \
      -d RBP={wildcards.RBP} \
      -d LAMBDA={wildcards.LAMBDA} \
      -d K={wildcards.K} \
      -d W={wildcards.W} \
      -d SIGMAcomp={wildcards.SIGMAcomp} \
      -d SIGMAmate={wildcards.SIGMAmate} \
      -d SIGMAdisp={wildcards.SIGMAdisp} \
      -d selfing={wildcards.selfing} \
      -d MAXT={wildcards.MAXT} \
      -d nrep={wildcards.nrep} \
      -d "output='{output}'" \
      {input[1]}
    """

# ------------------ get true tree-sequences and locations of sample, and make VCF -----------------
# runs in <2m w/ 80 threads, one each

true_trees = slim_trees.replace('.trees','_{Ne}Ne_{U}U_{k}k.{end}')
ends = ["locs", "trees", "vcf", "Ne"]

#rule true_trees:
#  input:
#    expand(true_trees, L=Ls, RBP=RBPs, LAMBDA=LAMBDAs, K=Ks, W=Ws, SIGMAcomp=SIGMAcomps, SIGMAmate=SIGMAmates, SIGMAdisp=SIGMAdisps, selfing=selfings, MAXT=MAXTs, nrep=nreps, Ne=Nes, U=Us, k=ks, end=ends)

rule true_tree:
  input:
    slim_trees 
  output:
    expand(true_trees, end=ends, allow_missing=True)
  group: "true_trees"
  threads: 1
  run:
    import tskit, pyslim, msprime
    import numpy as np
    ts = tskit.load(input[0]) #load tree sequence
    inds = np.random.choice(range(ts.num_individuals), int(wildcards.k), replace=False) #select k individuals randomly from current generation
    samples = [node for ind in inds for node in ts.individual(ind).nodes] #both genomes of each individual
    ts = ts.simplify(samples, keep_input_roots=True) #need to keep input roots for recapping
    ts = pyslim.recapitate(ts, recombination_rate=float(wildcards.RBP), ancestral_Ne=float(wildcards.Ne)) #recapitate
    ts.dump(output[1]) #save true treesequence
    locs = [ts.individual(ts.node(i).individual).location[:2] for i in ts.samples()] #locations of all sample nodes
    np.savetxt(output[0], locs) #save locations
    ts = msprime.sim_mutations(ts, rate=float(wildcards.U)) #layer on mutations
    with open(output[2], "w") as vcf_file:
      ts.write_vcf(vcf_file, individuals = range(int(wildcards.k)), individual_names = [str(inds[ts.node(i).individual]) for i in ts.samples()][::2]) #give the sampled individuals their original names, in case we need that later
    twoNe = ts.diversity() / (2 * float(wildcards.U)) #simple estimte of 2Ne
    np.savetxt(output[3], [int(twoNe)])

#TODO: add wildcard to sample 1 or 2 genomes per individual

# ------------------- convert vcf to haps/sample ----------------------
# seconds with 80 threads, one each

haps_sample = true_trees
ends = ['haps','sample']

#rule haps_samples: 
#  input:
#    expand(haps_sample, L=Ls, RBP=RBPs, LAMBDA=LAMBDAs, K=Ks, W=Ws, SIGMAcomp=SIGMAcomps, SIGMAmate=SIGMAmates, SIGMAdisp=SIGMAdisps, selfing=selfings, MAXT=MAXTs, nrep=nreps, Ne=Nes, U=Us, k=ks, end=ends)

rule haps_sample:
  input:
    expand(true_trees, end=['vcf'], allow_missing=True),
    RELATE
  output:
   expand(haps_sample, end=ends, allow_missing=True)
  params:
    prefix = true_trees.replace('.{end}','')
  group: "haps_sample"
  threads: 1
  shell:
    '''
    module load gcc/8.3.0 #needed for relate
    {RELATEDIR}/bin/RelateFileFormats \
                 --mode ConvertFromVcf \
                 --haps {output[0]} \
                 --sample {output[1]} \
                 -i {params.prefix} \
                 --chr 1
    '''   

# --------------- recombination map ---------
# instantaneous

rec_map = DATADIR + 'sim_{L}L_{RBP}RBP_{selfing}selfing.map'

#rule rec_maps:
#  input:
#    expand(rec_map, L=Ls, RBP=RBPs, selfing=selfings)

rule rec_map:
  input:
  output:
    rec_map
  threads: 1
  group: "rec_maps"
  run:
    import numpy as np
    L = int(wildcards.L)
    R = (1 - (1 - 2 * float(wildcards.RBP))**L)/2 #recombination distance from one end of chromosome to other
    cm = 50 * np.log(1/(1-2*R)) * (1-float(wildcards.selfing)) #length in centiMorgans, multiplied by outcrossing rate (effective length)
    cr = cm/L * 1e6 * (1-float(wildcards.selfing)) #effective cM per million bases
    script = "pos COMBINED_rate Genetic_Map \n0 %f 0 \n%d %f %f" %(cr, L, cr, cm)
    os.system("echo '" + script + "' >"  + output[0])

# --------------- haps to anc/mut ----------------
# <5m with 80 threds, 1 each

anc_mut = haps_sample
ends = ['anc','mut'] 

#rule anc_muts:
#  input:
#    expand(anc_mut, L=Ls, RBP=RBPs, LAMBDA=LAMBDAs, K=Ks, W=Ws, SIGMAcomp=SIGMAcomps, SIGMAmate=SIGMAmates, SIGMAdisp=SIGMAdisps, selfing=selfings, MAXT=MAXTs, nrep=nreps, Ne=Nes, U=Us, k=ks, end=ends)

rule anc_mut:
  input:
    expand(true_trees, end=['Ne'], allow_missing=True),
    expand(haps_sample, end=['haps','sample'], allow_missing=True),
    rec_map,
    RELATE
  output:
    expand(anc_mut, end=ends, allow_missing=True)
  threads: 1
  group: "anc_muts"
  params:
    prefix = anc_mut.replace(DATADIR,'').replace('.{end}',''),
  shell:
    '''
    twoNe=( $(cat {input[0]}) )
    module load gcc/8.3.0 #needed for relate
    {RELATEDIR}/bin/Relate \
      --mode All \
      -m {wildcards.U} \
      -N $twoNe \
      --haps {input[1]} \
      --sample {input[2]} \
      --map {input[3]} \
      --seed 1 \
      -o {params.prefix}
    mv {params.prefix}.* {DATADIR} 
    '''

# ------------- poplabels ------------
# instantaneous

poplabel = DATADIR + 'sim_{k}k.poplabels'

#rule poplabels:
#  input:
#    expand(poplabel, k=ks)

rule poplabel:
  input:
  output:
    poplabel
  threads: 1
  group: "poplabels"
  shell:
    '''
    echo "sample population group sex" > {output[0]}
    for i in {{1..{wildcards.k}}}; do echo "$i 1 1 NA" >> {output[0]}; done
    '''

# ------------- coalescence rates ---------
# note that I commented out the last line in Relate's EstimatePopulationSize.sh to avoid loading R to plot 
# 10m? with 80 threads, 1 each

coal_rate = anc_mut.replace('.{end}','_{numiter}numiter_{threshold}threshold.{end}')
ends = ['anc','mut','coal']

#rule coal_rates:
#  input:
#    expand(coal_rate, L=Ls, RBP=RBPs, LAMBDA=LAMBDAs, K=Ks, W=Ws, SIGMAcomp=SIGMAcomps, SIGMAmate=SIGMAmates, SIGMAdisp=SIGMAdisps, selfing=selfings, MAXT=MAXTs, nrep=nreps, Ne=Nes, U=Us, k=ks, numiter=numiters, threshold=thresholds, end=ends)

rule coal_rate:
  input:
    expand(anc_mut, end=['anc','mut'], allow_missing=True),
    poplabel
  output:
    expand(coal_rate, end=ends, allow_missing=True)
  threads: 1
  group: "coal_rates" 
  params:
    prefix = anc_mut.replace('.{end}',''),
    prefix_out = coal_rate.replace('.{end}','')
  shell:
    '''
    module load gcc/8.3.0 #needed for relate
    # edited relate script to supress plotting
    {RELATEDIR}/scripts/EstimatePopulationSize/EstimatePopulationSize.sh \
              -i {params.prefix} \
              -m {wildcards.U} \
              --years_per_gen 1 \
              --poplabels {input[2]} \
              --seed 1 \
              --num_iter {wildcards.numiter} \
              --threshold {wildcards.threshold} \
              -o {params.prefix_out} 
    '''
# ------------ decide which trees to sample ----------------
# instantaneous

bps = anc_mut.replace('.{end}','_{treeskip}treeskip.bps')

rule bps:
  input:
    expand(bps, L=Ls, RBP=RBPs, LAMBDA=LAMBDAs, K=Ks, W=Ws, SIGMAcomp=SIGMAcomps, SIGMAmate=SIGMAmates, SIGMAdisp=SIGMAdisps, selfing=selfings, MAXT=MAXTs, nrep=nreps, Ne=Nes, U=Us, k=ks, numiter=numiters, threshold=thresholds, treeskip=treeskips)

checkpoint bp:
  input:
    expand(anc_mut, end=['anc','mut'], allow_missing=True)
  output:
    bps 
  threads: 1
  group: "bps"
  run:
    print('getting tree indices')
    ixs_start=[]
    ixs_end=[]
    with open(input[0], "r") as f:
      for i, line in enumerate(f): #each line is a tree, see https://myersgroup.github.io/relate/getting_started.html#Output
        if i==1: 
          n = int(line.split()[1]) #number of trees on this chromosome
          trees = [i for i in range(0,n+1,int(wildcards.treeskip))] #which trees to sample
        if i > 1 and i-2 in trees:
          ixs_start.append(int(line.split(':')[0])) #index of first snp in sampled tree
        if i > 2 and i-3 in trees: 
          ixs_end.append(int(line.split(':')[0])-1) #index of last snp in sampled tree
    print('choose',len(ixs_start),'trees')
    print('getting start and stop basepairs')
    bps_start = []
    bps_end = []
    with open(input[1],"r") as f:
      for i,line in enumerate(f):
        if i>0 and int(line.split(';')[0]) in ixs_start:
          bps_start.append(int(line.split(';')[1])) #position of first snp in sampled tree
        if i>1 and int(line.split(';')[0]) in ixs_end:
          bps_end.append(int(line.split(';')[1])) #position of last snp in sampled tree
    print('writing to file')
    with open(output[0], 'w') as out:
      for start,end in zip(bps_start,bps_end):
        out.write(str(start) + ' ' + str(end) + '\n')

# ------------ sample branch lengths at a particular location -------------

tree_samples = coal_rate.replace('.{end}','_{treeskip}treeskip_{locus}locus_{M}M.newick')

rule sample_tree:
  input:
    bps,
    expand(coal_rate, end=['anc','mut','coal'], allow_missing=True)
  output:
    tree_samples
  params:
    prefix_in = coal_rate.replace('.{end}',''),
    prefix_out = tree_samples.replace('.newick','')
  threads: 1
  group: "sample_trees"
  shell:
    '''
    start=( $(awk 'FNR == {wildcards.locus} {{print $1}}' {input[0]}) )
    stop=( $(awk 'FNR == {wildcards.locus} {{print $2}}' {input[0]}) )
    module load gcc/9.2.0
    {RELATEDIR}/scripts/SampleBranchLengths/SampleBranchLengths.sh \
                 -i {params.prefix_in} \
                 --coal {input[3]} \
                 -o {params.prefix_out} \
                 -m {wildcards.U} \
                 --format n \
                 --num_samples {wildcards.M} \
                 --first_bp $start \
                 --last_bp  $stop \
                 --seed 1 
    '''

# --------------- get shared and coalescence times --------------------

times = tree_samples.replace('.newick','_{end}')
ends = ['sts.npy','cts.npy']

rule times:
  input:
    tree_samples
  output:
    expand(times, end=ends, allow_missing=True)
  threads: 1
  group: "times"
  run:
    from tsconvert import from_newick
    from utils import get_shared_times
    import numpy as np
    import os
    # tame numpy
    os.environ["OMP_NUM_THREADS"] = str(threads)
    os.environ["GOTO_NUM_THREADS"] = str(threads)
    os.environ["OPENBLAS_NUM_THREADS"] = str(threads)
    os.environ["MKL_NUM_THREADS"] = str(threads)
    os.environ["VECLIB_MAXIMUM_THREADS"] = str(threads)
    os.environ["NUMEXPR_NUM_THREADS"] = str(threads)
    stss = []
    ctss = []
    with open(input[0], mode='r') as f:
      next(f) #skip header
      for line in f: #for each tree sampled at a locus
        string = line.split()[4] #extract newick string only (Relate adds some info beforehand)
        ts = from_newick(string) #convert to tskit "tree sequence" (only one tree)
        tree = ts.first() #the only tree
        samples = [int(ts.node(node).metadata['name']) for node in ts.samples()] #get index of each sample in list we gave to relate
        sample_order = np.argsort(samples) #get indices to put in ascending order
        ordered_samples = [ts.samples()[i] for i in sample_order] #order samples as in relate
        sts = get_shared_times(tree, ordered_samples) #get shared times between all pairs of samples, with rows and columns ordered as in relate
        stss.append(sts)
        cts = sorted([tree.time(i) for i in tree.nodes() if not tree.is_sample(i)]) #coalescence times, in ascending order
        ctss.append(cts)
    np.save(output[0], np.array(stss))
    np.save(output[1], np.array(ctss))

# ------------------ process shared times --------------------

processed_shared_times = times.replace('{end}','{tCutoff}tCutoff_{end}')
ends = ['mc-invs.npy','logdets.npy','samples.npy']

rule process_shared_time:
  input:
    times.replace('{end}','sts.npy')
  output:
    expand(processed_shared_times, end=ends, allow_missing=True)
  threads: 1 
  group: "process_times"
  run:
    # tame numpy
    import os
    os.environ["OMP_NUM_THREADS"] = str(threads)
    os.environ["GOTO_NUM_THREADS"] = str(threads)
    os.environ["OPENBLAS_NUM_THREADS"] = str(threads)
    os.environ["MKL_NUM_THREADS"] = str(threads)
    os.environ["VECLIB_MAXIMUM_THREADS"] = str(threads)
    os.environ["NUMEXPR_NUM_THREADS"] = str(threads)
    import numpy as np
    from utils import chop_shared_times, center_shared_times
    # load times at all trees
    stss = np.load(input[0])
    _,n,_ = stss.shape
    # process trees
    stss_inv = []
    stss_logdet = []
    smplsss = []
    tCutoff = wildcards.tCutoff
    if tCutoff=='None': 
      tCutoff=None 
    else:
      tCutoff=float(tCutoff)
    for sts in stss:
      # chop
      sts_chopped, smpls = chop_shared_times(sts, tCutoff=tCutoff) #shared times and samples of each subtree
      sts_inv = []
      sts_logdet = []
      smplss = []
      # process subtrees
      for st,sm in zip(sts_chopped, smpls):
        stc = center_shared_times(st) #mean center
        stc_inv = np.linalg.inv(stc) #invert
        stc_logdet = np.linalg.slogdet(stc)[1] #log determinant
        sts_inv.append(stc_inv)
        sts_logdet.append(stc_logdet) 
        smplss.append(sm) #samples
      stss_inv.append(sts_inv)
      stss_logdet.append(sts_logdet)
      smplsss.append(smplss)
    # save
    np.save(output[0], np.array(stss_inv, dtype=object))
    np.save(output[1], np.array(stss_logdet, dtype=object))
    np.save(output[2], np.array(smplsss, dtype=object))                    

# ------------- process coalescence times ----------

processed_coal_times = times.replace('{end}','{tCutoff}tCutoff_{end}')
ends = ['bts.npy','lpcs.npy']

rule process_coal_time:
  input:
    times.replace('{end}','cts.npy'),
    coal_rate.replace('{end}','coal')
  output:
    expand(processed_coal_times, end=ends, allow_missing=True)
  threads: 1 
  group: "process_times"
  run:
    # taming numpy
    import os
    os.environ["OMP_NUM_THREADS"] = str(threads)
    os.environ["GOTO_NUM_THREADS"] = str(threads)
    os.environ["OPENBLAS_NUM_THREADS"] = str(threads)
    os.environ["MKL_NUM_THREADS"] = str(threads)
    os.environ["VECLIB_MAXIMUM_THREADS"] = str(threads)
    os.environ["NUMEXPR_NUM_THREADS"] = str(threads)
    import numpy as np
    from spacetrees import _log_coal_density
    # get variable Ne
    epochs = np.genfromtxt(input[1], skip_header=1, skip_footer=1) #time at which each epoch starts (and the final one ends)
    Nes = 0.5/np.genfromtxt(input[1], skip_header=2)[2:] #effective population size during each epoch
    # process coal times
    ctss = np.load(input[0]) #coalescence times in ascending order, for each tree
    btss = []
    lpcs = []
    tCutoff = wildcards.tCutoff
    if tCutoff=='None': 
      tCutoff=None 
    else:
      tCutoff=float(tCutoff)
    for cts in ctss: 
      # get branching times in ascending order
      T = cts[-1] #TMRCA (to be replaced with tCutoff)
      if tCutoff is not None:
        if tCutoff < T:
          T = tCutoff
      bts = T - np.flip(cts) #branching times, in ascending order
      bts = bts[bts>0] #remove branching times at or before T
      bts = np.append(bts,T) #append total time as last item
      btss.append(bts)
      # get probability of coalescence times under panmictic coalescent with variable Ne
      lpc = _log_coal_density(times=cts, Nes=Nes, epochs=epochs, tCutoff=tCutoff)
      lpcs.append(lpc)
    # save
    np.save(output[0], np.array(btss, dtype=object))
    np.save(output[1], np.array(lpcs, dtype=object))      

# ----------- composite dispersal rates ------------------------

composite_dispersal_rate = processed_shared_times.replace('_{locus}locus','').replace('{end}','mle-dispersal.npy')

rule composite_dispersal_rates:
  input:
    #expand(composite_dispersal_rate, L=Ls, RBP=RBPs, LAMBDA=LAMBDAs, K=Ks, W=Ws, SIGMAcomp=SIGMAcomps, SIGMAmate=SIGMAmates, SIGMAdisp=SIGMAdisps, selfing=selfings, MAXT=MAXTs, nrep=nreps, Ne=Nes, U=Us, k=ks, numiter=numiters, threshold=thresholds, treeskip=treeskips, M=Ms, tCutoff=tCutoffs)
    expand(composite_dispersal_rate, L=Ls, RBP=RBPs, LAMBDA=LAMBDAs, K=Ks, W=Ws, SIGMAcomp=SIGMAcomps, SIGMAmate=SIGMAmates, SIGMAdisp=[0.5], selfing=selfings, MAXT=MAXTs, nrep=[0], Ne=Nes, U=Us, k=ks, numiter=numiters, threshold=thresholds, treeskip=treeskips, M=Ms, tCutoff=[None])

def input_func(name,ends):
  def input_files(wildcards):
    filenames = []
    infile = checkpoints.bp.get(**wildcards).output[0]
    with open(infile,'r') as f:
      for i,_ in enumerate(f):
        string = name.replace('{locus}',str(i+1))
        filenames.append(string)
    return expand(filenames, end=ends, **wildcards)
  return input_files

rule composite_dispersal_rate:
  input:
    stss_mc_inv = input_func(processed_shared_times, ['mc-invs.npy']),
    stss_logdet = input_func(processed_shared_times, ['logdets.npy']),
    smplss = input_func(processed_shared_times, ['samples.npy']),
    btss = input_func(processed_coal_times, ['bts.npy']),
    lpcss = input_func(processed_coal_times, ['lpcs.npy']),
    locs = true_trees.replace('{end}','locs') 
  output:
    composite_dispersal_rate
  threads: 1
  group: "composite_dispersal_rates"
  run:
    # taming numpy
    import os
    os.environ["OMP_NUM_THREADS"] = str(threads)
    os.environ["GOTO_NUM_THREADS"] = str(threads)
    os.environ["OPENBLAS_NUM_THREADS"] = str(threads)
    os.environ["MKL_NUM_THREADS"] = str(threads)
    os.environ["VECLIB_MAXIMUM_THREADS"] = str(threads)
    os.environ["NUMEXPR_NUM_THREADS"] = str(threads)
    import numpy as np
    from spacetrees import mle_dispersal, _sds_rho_to_sigma
    from tqdm import tqdm
    # load locations
    locations = np.loadtxt(input.locs) #sample node locations
    # for testing
    L = len(input.stss_mc_inv)
    M = int(wildcards.M)
    L = 10 #number of loci
    M = 10 #number of trees per locus
    #mean centered and inverted shared time matrices
    print('\nloading inverted shared times matrices')
    stss_mc_inv = []
    for f in tqdm(input.stss_mc_inv[:L]):
      sts_mc_inv = np.load(f, allow_pickle=True)[:M]
      stss_mc_inv.append(sts_mc_inv)
    #log determinants of mean centered shared time matrices   
    print('\nloading log determinants of shared times matrices')
    stss_logdet = []
    for f in tqdm(input.stss_logdet[:L]):
      sts_logdet = np.load(f, allow_pickle=True)[:M]
      stss_logdet.append(sts_logdet) 
    #subtree samples
    print('\nloading samples of shared times matrices')
    smplss = []
    for f in tqdm(input.smplss[:L]):
      smpls = np.load(f, allow_pickle=True)[:M]
      smplss.append(smpls) 
    #branching times
    print('\nloading branching times')
    btss = []
    for f in tqdm(input.btss[:L]):
      bts = np.load(f, allow_pickle=True)[:M] 
      btss.append(bts)
    #log probability of coalescent times   
    print('\nloading log probability of coalescence times')
    lpcss = []
    for f in tqdm(input.lpcss[:L]):
      lpcs = np.load(f, allow_pickle=True)[:M] 
      lpcss.append(lpcs)
    # function for updates
    def callbackF(x):
      print('{0: 3.6f}   {1: 3.6f}   {2: 3.6f}   {3: 3.6f}'.format(x[0], x[1], x[2], x[3]))
    # find parameter estimates
    print('\nestimating dispersal rate')
    mle = mle_dispersal(locations=locations, shared_times_inverted=stss_mc_inv, log_det_shared_times=stss_logdet, samples=smplss, 
                        sigma0=None, phi0=None, #make educated guess based on first tree at each locus
                        callbackF=callbackF,
                        important=True, branching_times=btss, logpcoals=lpcss)
    print('\n',mle)
    np.save(output[0], mle) 

######################################################
## -------------- ancestor locations -----------------
######################################################
#
## ------- SLiM simulations with retained ancestors -------
#
#retain_slim_pattern = DATADIR + "retain_slim_{L}L_{RBP}RBP_{LAMBDA}LAMBDA_{K}K_{W}W_{SIGMAi}SIGMAi_{SIGMAd}SIGMAd_{MAXT}MAXT_{nrep}nrep.trees" #note that any changes to this require changes in the slim script
#
#retain_slim_results = expand(retain_slim_pattern, 
#		      L=Ls, RBP=RBPs, 
#                      LAMBDA=LAMBDAs, K=Ks,
#                      W=Ws, SIGMAi=SIGMAis,
#		      SIGMAd=anc_SIGMAds, MAXT=MAXTs, 
#		      nrep=anc_reps)
#
#rule all_retain_sims:
#  input:
#    retain_slim_results
#
#rule slim_retain_sim:
#  input:
#    "scripts/sim_retain.slim"
#  output:
#    retain_slim_pattern
#  wildcard_constraints:
#    nrep="\d+"
#  shell:
#    """
#    # make the data directory if it doesn't already exist
#    mkdir -p {DATADIR}
#
#    # the output files are automatically generated from the SLiM script
#    {SLIM} -d L={wildcards.L} -d RBP={wildcards.RBP} \
#     -d LAMBDA={wildcards.LAMBDA} -d K={wildcards.K} \
#     -d W={wildcards.W} -d SIGMAi={wildcards.SIGMAi} \
#     -d SIGMAd={wildcards.SIGMAd} -d MAXT={wildcards.MAXT} \
#     -d nrep={wildcards.nrep} -d "retain_gens={retain_gens_str}" \
#     {input}
#    """
#
## --------------- simplify, recapitate, mutate ------------
#
#tree_outputs = ["_true.trees", ".locs", ".vcf"]
#
#tree_pattern = retain_slim_pattern.replace('.trees','_{Ne}Ne_{U}U_{k}k_{d}d_sample{sim_output}')
#
#tree_results = expand(tree_pattern, 
#                      L=Ls, RBP=RBPs,
#                      LAMBDA=LAMBDAs, K=Ks,
#                      W=Ws, SIGMAi=SIGMAis,
#                      SIGMAd=anc_SIGMAds, MAXT=MAXTs,
#                      nrep=anc_reps, Ne=Nes, U=Us, 
#                      k=ks, 
#		      sim_output=tree_outputs)
#
#rule all_retain_true_trees:
#  input:
#    tree_results
#
#rule simplify_recap_mut:
#  input:
#    retain_slim_pattern
#  output:
#    expand(tree_pattern, sim_output=tree_outputs, allow_missing=True)
#  run:
#    outfile = output[0].replace(tree_outputs[0], "")
#    sparg.recap_mutate_sample(input[0], float(wildcards.RBP), float(wildcards.Ne), float(wildcards.U), int(wildcards.k), outfile, W=float(wildcards.W), d=float(wildcards.d), keep_unary=True)
#
## ------------------- make simple true tree sequence -----------------
#
#simple_tree_pattern = tree_pattern.replace('{sim_output}', '_true_simple.trees')
#
#simple_tree_results = expand(simple_tree_pattern,
#                      L=Ls, RBP=RBPs,
#                      LAMBDA=LAMBDAs, K=Ks,
#                      W=Ws, SIGMAi=SIGMAis,
#                      SIGMAd=anc_SIGMAds, MAXT=MAXTs,
#                      nrep=anc_reps, Ne=Nes, U=Us,
#                      k=ks) 
#
#rule all_make_simple_tree:
#  input:
#    simple_tree_results
#
#rule make_simple_tree:
#  input:
#    tree_pattern.replace('{sim_output}', tree_outputs[0]) #true tree
#  output:
#    simple_tree_pattern
#  run:
#    ts = pyslim.load(input[0])
#    ts = ts.simplify() #just remove unary nodes for downstream stuff
#    ts.dump(output[0])
#
## ------------------- infer the tree sequence ----------------------
#
#inf_tree_pattern = tree_pattern.replace('{sim_output}','_{num_iter}numiter_inferred.trees')
#
#inf_tree_results = expand(inf_tree_pattern, 
#                          L=Ls, RBP=RBPs,
#                          LAMBDA=LAMBDAs, K=Ks,
#                          W=Ws, SIGMAi=SIGMAis,
#                          SIGMAd=anc_SIGMAds, MAXT=MAXTs,
#                          nrep=anc_reps, Ne=Nes, U=Us, k=ks, 
#                          num_iter=num_iters)
#
#rule all_retain_inf_trees:
#  input:
#    inf_tree_results
#
#rule get_retain_inf_trees:
#  input:
#    tree_pattern.replace("{sim_output}", tree_outputs[0]) #use for name of true trees and vcf  
#  output:
#    inf_tree_pattern
#  threads: 80
#  run:
#    outfile = output[0].replace('.trees', '') #drop suffix
#    filename = input[0].replace(DATADIR,'').replace(tree_outputs[0],'') #drop directory and suffix (again, picky function that I should improve)
#    sparg.infer_ts(DATADIR, filename, RELATE, float(wildcards.RBP), int(wildcards.L), float(wildcards.U), int(wildcards.k), threshold=0, num_iter=int(wildcards.num_iter), memory=150, nthreads=threads, outfile=outfile, true_trees_file=input[0])
#
## ---------------- reorder locations for inferred trees ---------------------------
#
#reorder_locations_pattern =  tree_pattern.replace("{sim_output}", '_reordered.locs')
#
#reorder_locations_results = expand(reorder_locations_pattern,
#                                     L=Ls, RBP=RBPs,
#                                     LAMBDA=LAMBDAs, K=Ks,
#                                     W=Ws, SIGMAi=SIGMAis,
#                                     SIGMAd=anc_SIGMAds, MAXT=MAXTs,
#                                     nrep=anc_reps, Ne=Nes, U=Us, k=ks)
#
#rule all_reorder_locations_anc:
#  input:
#    reorder_locations_results
#
#rule reorder_locations_anc:
#  input:
#    simple_tree_pattern, #true tree to get node order
#    tree_pattern.replace("{sim_output}", tree_outputs[1]) #original order of locations
#  output:
#    reorder_locations_pattern
#  run:
#    ts = tskit.load(input[0])
#    node_list = []
#    for ind in ts.individuals():
#      if ind.id in range(int(wildcards.k)):
#        node_list.append(ind.nodes)
#    node_list = np.array(node_list)
#    ind_locs = np.loadtxt(input[1])
#    locations = np.repeat(ind_locs, 2, axis=0) 
#    locations = locations[node_list.flatten()]
#    np.savetxt(output[0], locations)
#
## --------------------- choose loci to sample from true trees  --------------------------------
#
#loci_pattern = tree_pattern.replace('{sim_output}','_true_%dnloci.npz' %nloci)
#
#loci_results = expand(loci_pattern,
#                      L=Ls, RBP=RBPs,
#                      LAMBDA=LAMBDAs, K=Ks,
#                      W=Ws, SIGMAi=SIGMAis,
#                      SIGMAd=anc_SIGMAds, MAXT=MAXTs,
#                      nrep=anc_reps, Ne=Nes, U=Us,
#                      k=ks)
#
#rule all_choose_loci_anc:
#  input:
#    loci_results
#
#rule choose_loci_anc:
#  input:
#    simple_tree_pattern
#  output:
#    loci_pattern
#  run:
#    ts = tskit.load(input[0]) #load tree sequence
#    which_trees, intervals = sparg.choose_loci(ts, which=anc_which_sites, mode='site') #determine tree indices and genomic intervals of each
#    np.savez(output[0], which_trees=which_trees, intervals=intervals) #save for downstream
#
## --------------------- choose loci to sample from inferred trees  --------------------------------
#
#inf_loci_pattern = inf_tree_pattern.replace('.trees','_%dnloci.npz' %nloci)
#
#inf_loci_results = expand(inf_loci_pattern,
#                          L=Ls, RBP=RBPs,
#                          LAMBDA=LAMBDAs, K=Ks,
#                          W=Ws, SIGMAi=SIGMAis,
#                          SIGMAd=anc_SIGMAds, MAXT=MAXTs,
#                          nrep=anc_reps, Ne=Nes, U=Us,
#                          k=ks, 
#                          num_iter=num_iters)
#
#rule all_choose_inf_loci_anc:
#  input:
#    inf_loci_results
#
#rule choose_inf_loci_anc:
#  input:
#    inf_tree_pattern
#  output:
#    inf_loci_pattern
#  run:
#    ts = tskit.load(input[0]) #load tree sequence
#    which_trees, intervals = sparg.choose_loci(ts, which=anc_which_sites, mode='site') #determine tree indices and genomic intervals of each
#    np.savez(output[0], which_trees=which_trees, intervals=intervals) #save for downstream
#
## --------------------- choose loci to sample from true (unsimplified) trees  --------------------------------
#
#true_loci_pattern = tree_pattern.replace('{sim_output}','_big_true_%dnloci.npz' %nloci)
#
#true_loci_results = expand(true_loci_pattern,
#                      L=Ls, RBP=RBPs,
#                      LAMBDA=LAMBDAs, K=Ks,
#                      W=Ws, SIGMAi=SIGMAis,
#                      SIGMAd=anc_SIGMAds, MAXT=MAXTs,
#                      nrep=anc_reps, Ne=Nes, U=Us,
#                      k=ks )
#
#rule all_choose_true_loci:
#  input:
#    true_loci_results
#
#rule choose_true_loci:
#  input:
#    tree_pattern.replace('{sim_output}',tree_outputs[0])
#  output:
#    true_loci_pattern
#  run:
#    ts = tskit.load(input[0]) #load tree sequence
#    which_trees, intervals = sparg.choose_loci(ts, which=anc_which_sites, mode='site') #determine tree indices and genomic intervals of each
#    np.savez(output[0], which_trees=which_trees, intervals=intervals) #save for downstream
#
## ---------------------- process true trees -------------------------------
#
#process_trees_pattern = loci_pattern.replace('.npz','_processed_{locus}locus_{tCutoff}tCutoff.npz')
#
#process_trees_results = expand(process_trees_pattern,
#                               L=Ls, RBP=RBPs,
#                               LAMBDA=LAMBDAs, K=Ks,
#                               W=Ws, SIGMAi=SIGMAis,
#                               SIGMAd=anc_SIGMAds, MAXT=MAXTs,
#                               nrep=anc_reps, Ne=Nes, U=Us,
#                               k=ks, 
#                               locus=loci, tCutoff=anc_tCutoffs)
#
#rule all_process_trees_anc:
#  input:
#    process_trees_results
#
#rule process_trees_anc:
#  input:
#    loci_pattern,
#    simple_tree_pattern
#  output:
#    process_trees_pattern
#  run:
#    # load list of trees we are going to sample
#    npz = np.load(input[0], allow_pickle=True)
#    which_trees = npz['which_trees']
#    # and choose one to process
#    i = int(wildcards.locus)
#    locus = which_trees[i]
#    print('tree index:', locus)
#    # process
#    ts = tskit.load(input[1])
#    coal_times, pcoals, shared_times, samples = sparg.process_trees(which_loci=[locus],
#                                                                    tCutoff=int(wildcards.tCutoff),
#                                                                    ts = ts)
#    # save
#    np.savez(output[0], coal_times=coal_times, pcoals=pcoals, shared_times=shared_times, samples=samples)
#
## ---------------------- process inferred trees -------------------------------
#
#process_inf_trees_pattern = inf_loci_pattern.replace('.npz','_processed_{locus}locus_{tCutoff}tCutoff_{M}M.npz')
#sub_pattern = inf_loci_pattern.replace('.npz','_sub_{locus}locus.newick')
#
#process_inf_trees_results = expand(process_inf_trees_pattern,
#                                   L=Ls, RBP=RBPs,
#                                   LAMBDA=LAMBDAs, K=Ks,
#                                   W=Ws, SIGMAi=SIGMAis,
#                                   SIGMAd=anc_SIGMAds, MAXT=MAXTs,
#                                   nrep=anc_reps, Ne=Nes, U=Us,
#                                   k=ks, 
#                                   num_iter=num_iters,
#                                   locus=loci, tCutoff=anc_tCutoffs, M=Ms)
#
#rule all_process_inf_trees_anc:
#  input:
#    process_inf_trees_results
#
#rule process_inf_trees_anc:
#  input:
#    inf_loci_pattern,
#    inf_tree_pattern #don't actually use the tree sequence, just use filename to access anc/mut files
#  output:
#    process_inf_trees_pattern
#  run:
#    # load list of trees we are going to sample
#    npz = np.load(input[0], allow_pickle=True)
#    which_trees = npz['which_trees']
#    intervals = npz['intervals']
#    # and choose one to process
#    i = int(wildcards.locus)
#    locus = which_trees[i]
#    print('tree index:', locus)
#    interval = intervals[i]
#    print('genomic interval:', interval)
#    # process
#    filename = input[1].replace('.trees','')    
#    coal_times, pcoals, shared_times, samples = sparg.process_trees(which_loci=[locus],
#                                                                    intervals=[interval],
#                                                                    tCutoff=int(wildcards.tCutoff),
#                                                                    important=True,
#                                                                    M=int(wildcards.M),
#                                                                    PATH_TO_RELATE=RELATE,
#                                                                    u=float(wildcards.U),
#                                                                    infile=filename,
#                                                                    coalfile=filename + '.coal',
#                                                                    outfile=filename + '_sub')
#    # save
#    np.savez(output[0], coal_times=coal_times, pcoals=pcoals, shared_times=shared_times, samples=samples)
#
## --------------------- MCLEs of dispseral rate from true trees ----------------------
#
#mcles_pattern = process_trees_pattern.replace('.npz', '_mcle_dispersal.npy').replace('_{locus}locus','')
#
#mcles_results = expand(mcles_pattern,
#                      L=Ls, RBP=RBPs,
#                      LAMBDA=LAMBDAs, K=Ks,
#                      W=Ws, SIGMAi=SIGMAis,
#                      SIGMAd=anc_SIGMAds, MAXT=MAXTs,
#                      nrep=anc_reps, Ne=Nes, U=Us, k=ks, 
#                      tCutoff=anc_tCutoffs)
#
#rule all_retain_mcles:
#  input:
#    mcles_results
#
#rule retain_mcles:
#  input:
#    expand(process_trees_pattern, locus=loci, allow_missing=True), #processed treees
#    tree_pattern.replace("{sim_output}", tree_outputs[1]) #locations
#  output:
#    mcles_pattern
#  threads: 40
#  run:
#    # load processed trees
#    coal_times = []
#    pcoals = []
#    shared_times = []
#    samples = []
#   # for chromosome in [wildcards.CHR]: #chromosomes: #running it chromosome by chromosome because takes >24 hours for whole genome
#    for locus in loci:
#      #processed_trees = np.load(input[(chromosome-1)*nloci + locus], allow_pickle=True) #find correct input file from expansion
#      processed_trees = np.load(input[locus], allow_pickle=True) #find correct input file from expansion
#      coal_times.append(processed_trees['coal_times'][0])
#      pcoals.append(processed_trees['pcoals'][0])
#      shared_times.append(processed_trees['shared_times'][0])
#      samples.append(processed_trees['samples'][0])
#    # get locations of samples
#    ind_locs = np.loadtxt(input[-1]) #locations of diploid individuals
#    locations = np.repeat(ind_locs, 2, axis=0)[:,0:2] #chromosome locations in 2D
#    # get mle dispersal rate
#    x0 = np.array([float(wildcards.SIGMAd), float(wildcards.SIGMAd), 0, 0.001]) #initial guess for dispersal SDs, correlation, and branching rate
#    mle = sparg.find_mle(locations, coal_times, pcoals, shared_times, samples, x0=x0, bnds=bnds, tCutoff=int(wildcards.tCutoff), important=False, quiet=False, scale_phi=scale_phi, remove_missing=False, method=method)
#    if mle.success: 
#      np.save(output[0], mle)
#    else:
#      print('failed!')
#
## --------------------- MLEs of dispseral rate from inferred trees ----------------------
#
#inf_mcles_pattern = process_inf_trees_pattern.replace('.npz', '_mcle_dispersal.npy').replace('_{locus}locus','')
#
#inf_mcles_results = expand(inf_mcles_pattern,
#                          L=Ls, RBP=RBPs,
#                          LAMBDA=LAMBDAs, K=Ks,
#                          W=Ws, SIGMAi=SIGMAis,
#                          SIGMAd=anc_SIGMAds, MAXT=MAXTs,
#                          nrep=anc_reps, Ne=Nes, U=Us, k=ks, 
#                          num_iter=num_iters,
#                          tCutoff=anc_tCutoffs, M=Ms)
#
#rule all_retain_inf_mcles:
#  input:
#    inf_mcles_results
#
#rule retain_inf_mcles:
#  input:
#    expand(process_inf_trees_pattern, locus=loci, allow_missing=True),#processed treees
#    reorder_locations_pattern #locations
#  output:
#    inf_mcles_pattern
#  threads: 40
#  resources:
#    time = 30
#  run:
#    # load processed trees
#    coal_times = []
#    pcoals = []
#    shared_times = []
#    samples = []
#   # for chromosome in [wildcards.CHR]: #chromosomes: #running it chromosome by chromosome because takes >24 hours for whole genome
#    for locus in loci:
#      #processed_trees = np.load(input[(chromosome-1)*nloci + locus], allow_pickle=True) #find correct input file from expansion
#      processed_trees = np.load(input[locus], allow_pickle=True) #find correct input file from expansion
#      coal_times.append(processed_trees['coal_times'][0])
#      pcoals.append(processed_trees['pcoals'][0])
#      shared_times.append(processed_trees['shared_times'][0])
#      samples.append(processed_trees['samples'][0])
#    # get locations of samples
#    locations = np.loadtxt(input[-1])[:,:2] #locations of haploid genomes
#    # get mle dispersal rate
#    x0 = np.array([float(wildcards.SIGMAd), float(wildcards.SIGMAd), 0, 0.001]) #initial guess for dispersal SDs, correlation, and branching rate
#    mle = sparg.find_mle(locations, coal_times, pcoals, shared_times, samples, x0=x0, bnds=bnds, tCutoff=int(wildcards.tCutoff), important=True, quiet=False, scale_phi=scale_phi, remove_missing=False, method=method)
#    if not mle.success:
#      new_scale_phi = scale_phi * 10 #increase scale_phi by order of magnitude
#      print('search failed with scale_phi=%d, trying scale_phi=%d...' %(scale_phi,new_scale_phi))
#      #x0 = mle.x #take where we left off
#      x0[-1] = x0[-1] * new_scale_phi/scale_phi #rescale starting guess for phi
#      mle = sparg.find_mle(locations, coal_times, pcoals, shared_times, samples, x0=x0, bnds=bnds, tCutoff=int(wildcards.tCutoff), important=True, quiet=False, scale_phi=new_scale_phi, remove_missing=False, method=method, style=wildcards.style)
#    if mle.success: 
#      np.save(output[0], mle)
#    else:
#      print('still failed!')
#
## ---------------- ancestor locations from true trees ------------------------
#
#true_mle_locs_pattern = mcles_pattern.replace('mcle_dispersal','mle-locs_{locus}locus_{loctCutoff}loc-tCutoff') 
#
#true_mle_locs_results = expand(true_mle_locs_pattern, 
#                               L=Ls, RBP=RBPs,
#                               LAMBDA=LAMBDAs, K=Ks,
#                               W=Ws, SIGMAi=SIGMAis,
#                               SIGMAd=anc_SIGMAds, MAXT=MAXTs,
#                               nrep=anc_reps, Ne=Nes, U=Us, k=ks, 
#                               locus=loci, tCutoff=anc_tCutoffs,
#                               loctCutoff=anc_loctCutoffs)
#
#rule all_anc_locs_true_trees:
#  input:
#    true_mle_locs_results
#
#rule anc_locs_true_trees:
#  input:
#    process_trees_pattern.replace('{tCutoff}','{loctCutoff}'), #processed trees
#    mcles_pattern, #mcle dispersal
#    tree_pattern.replace("{sim_output}", tree_outputs[1]), #locations
#  output:
#    true_mle_locs_pattern
#  threads: 40 #i thought it was minimize that used 40 threads, but actually appears to be numpy (matrix multiplication?)
#  run:
#    # required files
#    treefiles = [input[0]] #processed trees (add brackets because locate method for multiple loci)
#    mlefiles = [input[1]] #mle dispersal and branching rates
#    # nodes we wish to locate
#    nodes = range(2*int(wildcards.k)) #all the nodes 
#    #locations
#    ind_locs = np.loadtxt(input[2]) #locations of diploid individuals
#    locations = np.repeat(ind_locs, 2, axis=0)[:,:2] #chromosome locations in 2D
#    # naive guess for where they are
#    x0_locations = np.mean(locations, axis=0) #mean location of samples
#    bnds_locations = ((0,int(wildcards.W)),(0,int(wildcards.W))) #bounds on location
#    # find mle locations at each locus
#    mle_locations = sparg.locate(treefiles=treefiles, mlefiles=mlefiles, nodes=nodes, locations=locations, x0=x0_locations, bnds=bnds_locations, times=anc_times, weight=False, importance=False, tCutoff=int(wildcards.loctCutoff))
#    np.save(output[0], mle_locations)
#
## ---------------- ancestor locations from inferred trees ------------------------
#
#inf_mle_locs_pattern = inf_mcles_pattern.replace('dispersal','{style}style-locs_{locus}locus_{loctCutoff}loc-tCutoff') 
#
#inf_mle_locs_results = expand(inf_mle_locs_pattern, 
#                               L=Ls, RBP=RBPs,
#                               LAMBDA=LAMBDAs, K=Ks,
#                               W=Ws, SIGMAi=SIGMAis,
#                               SIGMAd=anc_SIGMAds, MAXT=MAXTs,
#                               nrep=anc_reps, Ne=Nes, U=Us, k=ks, 
#                               num_iter=num_iters,
#                               locus=loci, tCutoff=anc_tCutoffs, M=Ms,
#                               loctCutoff=anc_loctCutoffs,
#                               style=styles)
#
#rule all_anc_locs_inf_trees:
#  input:
#    inf_mle_locs_results
#
#rule anc_locs_inf_trees:
#  input:
#    process_inf_trees_pattern.replace('{tCutoff}','{loctCutoff}'), #processed trees
#    inf_mcles_pattern, #mle dispersal
#    reorder_locations_pattern #locations
#  output:
#    inf_mle_locs_pattern
#  threads: 40
#  resources:
#    time = 15
#  run:
#    # required files
#    treefiles = [input[0]] #processed trees (add brackets because locate method for multiple loci)
#    mlefiles = [input[1]] #mle dispersal and branching rates
#    # nodes we wish to locate
#    nodes = range(2*int(wildcards.k)) #all the nodes 
#    #locations
#    locations = np.loadtxt(input[2])[:,:2] #locations of haploid individuals
#    # naive guess for where they are
#    x0_locations = np.mean(locations, axis=0) #mean location of samples
#    bnds_locations = ((0,int(wildcards.W)),(0,int(wildcards.W))) #bounds on location
#    # decide if search over full log likelihood or just sample MLEs
#    BLUP = False
#    if wildcards.style == 'BLUP':
#      BLUP=True
#    # find mle locations at each locus
#    mle_locations = sparg.locate(treefiles=treefiles, mlefiles=mlefiles, nodes=nodes, locations=locations, x0=x0_locations, bnds=bnds_locations, times=anc_times, weight=False, importance=True, tCutoff=int(wildcards.loctCutoff), BLUP=BLUP)
#    np.save(output[0], mle_locations)
#
## ---------------- true ancestor locations ------------------------
#
#true_locs_pattern = tree_pattern.replace('{sim_output}', '_true_anc_locs.npy') 
#
#true_locs_results = expand(true_locs_pattern, 
#                           L=Ls, RBP=RBPs,
#                           LAMBDA=LAMBDAs, K=Ks,
#                           W=Ws, SIGMAi=SIGMAis,
#                           SIGMAd=anc_SIGMAds, MAXT=MAXTs,
#                           nrep=anc_reps, Ne=Nes, U=Us, k=ks)
#
#rule all_true_locs:
#  input:
#    true_locs_results
#    
#rule true_locs:
#  input:
#    tree_pattern.replace('{sim_output}',tree_outputs[0]), #true tree with unary nodes 
#    true_loci_pattern #which loci to look at
#  output:
#    true_locs_pattern 
#  run:
#    ts = pyslim.load(input[0]) #tree sequence with retained unary ancestors
#    focal_nodes = ts.samples() #all the nodes
#    # load list of trees we are going to sample
#    npz = np.load(input[1], allow_pickle=True)
#    loci = npz['which_trees']
#    # find true locations
#    true_locs = sparg.get_true_ancestral_locations(treeseq=ts, focal_nodes=focal_nodes, loci=loci, times=anc_times) 
#    np.save(output[0], true_locs)
#
###################################################
## -------------- 2 epochs -------------------------
###################################################
#
## ---------------- sims -----------------------
#
#slim_pattern = DATADIR + "slim_{L}L_{RBP}RBP_{LAMBDA}LAMBDA_{K}K_{W}W_{SIGMAi}SIGMAi_{SIGMAd1}SIGMAd1_{SIGMAd2}SIGMAd2_{T12}T12_{MAXT}MAXT_{nrep}rep.trees"
#
#slim_results = expand(slim_pattern,
#                       L=Ls, RBP=RBPs,
#                       LAMBDA=LAMBDAs, K=epoch_Ks,
#                       W=Ws, SIGMAi=SIGMAis,
#                       SIGMAd1=SIGMAd1s, SIGMAd2=SIGMAd2s,
#                       T12 = T12s, MAXT=MAXTs,
#                       nrep=epoch_reps)
#
## dummy rule to run all the sims 
#rule all_sim_epochs:
#  input:
#    slim_results
#
## run a sim
#rule slim_sim_epochs:
#  input:
#    "scripts/sim_2epochs.slim"
#  output:
#   slim_pattern
#  wildcard_constraints:
#    nrep="\d+"
#  resources:
#    time = 60
#  shell:
#    """
#    mkdir -p {DATADIR}
#
#    # the output files are automatically generated from the SLiM script
#    {SLIM} -d L={wildcards.L} -d RBP={wildcards.RBP} \
#     -d LAMBDA={wildcards.LAMBDA} -d K={wildcards.K} \
#     -d W={wildcards.W} -d SIGMAi={wildcards.SIGMAi} \
#     -d SIGMAd1={wildcards.SIGMAd1} -d SIGMAd2={wildcards.SIGMAd2} \
#     -d T12={wildcards.T12} -d MAXT={wildcards.MAXT} \
#     -d nrep={wildcards.nrep} {input}
#    """
#
## ------------------ get true tree-sequences (and locations) of sample, and make VCF -----------------
#
#tree_pattern = slim_pattern.replace('.trees','_{Ne}Ne_{U}U_{k}k_{d}d_sample{sim_output}')
#
#tree_results = expand(tree_pattern, 
#                             L=Ls, RBP=RBPs,
#                             LAMBDA=LAMBDAs, K=epoch_Ks,
#                             W=Ws, SIGMAi=SIGMAis,
#                             SIGMAd1=SIGMAd1s, SIGMAd2=SIGMAd2s,
#                             T12=T12s, MAXT=MAXTs,
#                             nrep=epoch_reps, Ne=epoch_Nes, U=Us, k=ks, 
#		             sim_output=tree_outputs)
#
## dummy rule to get trees from all sims
#rule all_true_trees_epochs:
#  input:
#    tree_results
#
## get tree from a sim
#rule get_true_trees_and_locations_epochs:
#  input:
#    slim_trees 
#  output:
#    expand(tree_pattern, sim_output=tree_outputs, allow_missing=True)
#  run:
#    outfile = output[0].replace(tree_outputs[0], "")
#    sparg.recap_mutate_sample(input[0], float(wildcards.RBP), float(wildcards.Ne), float(wildcards.U), int(wildcards.k), outfile, W=float(wildcards.W), d=float(wildcards.d))
#
## ------------------- infer the tree sequence ----------------------
#
#inf_tree_pattern = tree_pattern.replace('{sim_output}','_{num_iter}numiter_inferred.trees')
#
#inf_tree_results = expand(inf_tree_pattern, 
#                          L=Ls, RBP=RBPs,
#                          LAMBDA=LAMBDAs, K=epoch_Ks,
#                          W=Ws, SIGMAi=SIGMAis,
#                          SIGMAd1=SIGMAd1s, SIGMAd2=SIGMAd2s,
#                          T12=T12s, MAXT=MAXTs,
#                          nrep=epoch_reps, Ne=epoch_Nes, U=Us, k=ks, 
#                          num_iter=num_iters)
#
## dummy rule to get trees from all sims
#rule all_inf_trees_epochs:
#  input:
#    inf_tree_results
#
## get a treeseq from a vcf
#rule get_inf_trees_epochs:
#  input:
#    tree_pattern.replace("{sim_output}", tree_outputs[0]) 
#  output:
#    inf_tree_pattern
#  threads: 80
#  resources:
#    time = 30
#  run:
#    outfile = output[0].replace('.trees', '') #drop suffixV
#    filename = input[0].replace(DATADIR,'').replace(tree_outputs[0],'') #drop directory and suffix (again, picky function that I should improve)
#    sparg.infer_ts(DATADIR, filename, RELATE, float(wildcards.RBP), int(wildcards.L), float(wildcards.U), int(wildcards.k), threshold=0, num_iter=int(wildcards.num_iter), memory=150, nthreads=threads, outfile=outfile, true_trees_file=input[0])
#
## ---------------- reorder locations for inferred trees ---------------------------
#
#reorder_locations_pattern =  tree_pattern.replace("{sim_output}", '_reordered.locs')
#
#reorder_locations_results = expand(reorder_locations_pattern,
#                                     L=Ls, RBP=RBPs,
#                                     LAMBDA=LAMBDAs, K=epoch_Ks,
#                                     W=Ws, SIGMAi=SIGMAis,
#                                     SIGMAd1=SIGMAd1s, SIGMAd2=SIGMAd2s,
#                                     T12=T12s, MAXT=MAXTs,
#                                     nrep=epoch_reps, Ne=epoch_Nes, U=Us, k=ks, 
#                                     num_iter=num_iters)
#
#rule all_reorder_locations_epochs:
#  input:
#    reorder_locations_results
#
#rule reorder_locations_epochs:
#  input:
#    tree_pattern.replace("{sim_output}", tree_outputs[0]), #true tree to get node order
#    tree_pattern.replace("{sim_output}", tree_outputs[1]) #original order of locations
#  output:
#    reorder_locations_pattern
#  run:
#    ts = tskit.load(input[0])
#    node_list = []
#    for ind in ts.individuals():
#      if ind.id in range(int(wildcards.k)):
#        node_list.append(ind.nodes)
#    node_list = np.array(node_list)
#    ind_locs = np.loadtxt(input[1])
#    locations = np.repeat(ind_locs, 2, axis=0) 
#    locations = locations[node_list.flatten()]
#    np.savetxt(output[0], locations)
#
## --------------------- choose loci to sample from true trees  --------------------------------
#
#loci_pattern = tree_pattern.replace('{sim_output}','_true_%dnloci.npz' %nloci)
#
#loci_results = expand(loci_pattern,
#                      L=Ls, RBP=RBPs,
#                      LAMBDA=LAMBDAs, K=epoch_Ks,
#                      W=Ws, SIGMAi=SIGMAis,
#                      SIGMAd1=SIGMAd1s, SIGMAd2=SIGMAd2s,
#                      T12=T12s, MAXT=MAXTs,
#                      nrep=epoch_reps, Ne=epoch_Nes, U=Us, k=ks)
#
#rule all_choose_loci_epochs:
#  input:
#    loci_results
#
#rule choose_loci_epochs:
#  input:
#    tree_pattern.replace("{sim_output}", tree_outputs[0]), #true tree
#  output:
#    loci_pattern
#  run:
#    ts = tskit.load(input[0]) #load tree sequence
#    which_trees, intervals = sparg.choose_loci(ts, which=anc_which_sites, mode='site') #determine tree indices and genomic intervals of each
#    np.savez(output[0], which_trees=which_trees, intervals=intervals) #save for downstream
#
## --------------------- choose loci to sample from inferred trees  --------------------------------
#
#inf_loci_pattern = inf_tree_pattern.replace('.trees','_%dnloci.npz' %nloci)
#
#inf_loci_results = expand(inf_loci_pattern,
#                          L=Ls, RBP=RBPs,
#                          LAMBDA=LAMBDAs, K=epoch_Ks,
#                          W=Ws, SIGMAi=SIGMAis,
#                          SIGMAd1=SIGMAd1s, SIGMAd2=SIGMAd2s,
#                          T12=T12s, MAXT=MAXTs,
#                          nrep=epoch_reps, Ne=epoch_Nes, U=Us, k=ks, 
#                          num_iter=num_iters)
#
#rule all_choose_inf_loci_epochs:
#  input:
#    inf_loci_results
#
#rule choose_inf_loci_epochs:
#  input:
#    inf_tree_pattern
#  output:
#    inf_loci_pattern
#  run:
#    ts = tskit.load(input[0]) #load tree sequence
#    which_trees, intervals = sparg.choose_loci(ts, which=anc_which_sites, mode='site') #determine tree indices and genomic intervals of each
#    np.savez(output[0], which_trees=which_trees, intervals=intervals) #save for downstream
#
## ---------------------- process true trees -------------------------------
#
#process_trees_pattern = loci_pattern.replace('.npz','_processed_{locus}locus_{tCutoff}tCutoff.npz')
#
#process_trees_results = expand(process_trees_pattern,
#                               L=Ls, RBP=RBPs,
#                               LAMBDA=LAMBDAs, K=epoch_Ks,
#                               W=Ws, SIGMAi=SIGMAis,
#                               SIGMAd1=SIGMAd1s, SIGMAd2=SIGMAd2s,
#                               T12=T12s, MAXT=MAXTs,
#                               nrep=epoch_reps, Ne=epoch_Nes, U=Us, k=ks, 
#                               num_iter=num_iters,
#                               locus=loci, tCutoff=epoch_tCutoffs)
#
#rule all_process_trees_epochs:
#  input:
#    process_trees_results
#
#rule process_trees_epochs:
#  input:
#    loci_pattern,
#    tree_pattern.replace("{sim_output}", tree_outputs[0]) #true tree
#  output:
#    process_trees_pattern
#  run:
#    # load list of trees we are going to sample
#    npz = np.load(input[0], allow_pickle=True)
#    which_trees = npz['which_trees']
#    # and choose one to process
#    i = int(wildcards.locus)
#    locus = which_trees[i]
#    print('tree index:', locus)
#    # process
#    ts = tskit.load(input[1])
#    coal_times, pcoals, shared_times, samples = sparg.process_trees(which_loci=[locus],
#                                                                    tCutoff=int(wildcards.tCutoff),
#                                                                    ts = ts)
#    # save
#    np.savez(output[0], coal_times=coal_times, pcoals=pcoals, shared_times=shared_times, samples=samples)
#
## ---------------------- process inferred trees -------------------------------
#
#process_inf_trees_pattern = inf_loci_pattern.replace('.npz','_processed_{locus}locus_{tCutoff}tCutoff_{M}M.npz')
#
#process_inf_trees_results = expand(process_inf_trees_pattern,
#                                   L=Ls, RBP=RBPs,
#                                   LAMBDA=LAMBDAs, K=epoch_Ks,
#                                   W=Ws, SIGMAi=SIGMAis,
#                                   SIGMAd1=SIGMAd1s, SIGMAd2=SIGMAd2s,
#                                   T12=T12s, MAXT=MAXTs,
#                                   nrep=epoch_reps, Ne=epoch_Nes, U=Us, k=ks, 
#                                   num_iter=num_iters,
#                                   locus=loci, tCutoff=epoch_tCutoffs, M=Ms)
#
#rule all_process_inf_trees_epochs:
#  input:
#    process_inf_trees_results
#
#rule process_inf_trees_epochs:
#  input:
#    inf_loci_pattern,
#    inf_tree_pattern #don't actually use the tree sequence, just use filename to access anc/mut files
#  output:
#    process_inf_trees_pattern
#  run:
#    # load list of trees we are going to sample
#    npz = np.load(input[0], allow_pickle=True)
#    which_trees = npz['which_trees']
#    intervals = npz['intervals']
#    # and choose one to process
#    i = min(int(wildcards.locus), len(which_trees)-1) #for rare cases where we cut the chromosome off too short)
#    locus = which_trees[i]
#    print('tree index:', locus)
#    interval = intervals[i]
#    print('genomic interval:', interval)
#    # process
#    filename = input[1].replace('.trees','')    
#    coal_times, pcoals, shared_times, samples = sparg.process_trees(which_loci=[locus],
#                                                                    intervals=[interval],
#                                                                    tCutoff=int(wildcards.tCutoff),
#                                                                    important=True,
#                                                                    M=int(wildcards.M),
#                                                                    PATH_TO_RELATE=RELATE,
#                                                                    u=float(wildcards.U),
#                                                                    infile=filename,
#                                                                    coalfile=filename + '.coal',
#                                                                    outfile=filename + '_sub')
#    # save
#    np.savez(output[0], coal_times=coal_times, pcoals=pcoals, shared_times=shared_times, samples=samples)
#
## --------------------- MLEs of dispseral rate from true trees ----------------------
#
#mles_pattern = process_trees_pattern.replace('.npz', '_mle_dispersal.npy')
#
#mles_results = expand(mles_pattern,
#                      L=Ls, RBP=RBPs,
#                      LAMBDA=LAMBDAs, K=epoch_Ks,
#                      W=Ws, SIGMAi=SIGMAis,
#                      SIGMAd1=SIGMAd1s, SIGMAd2=SIGMAd2s,
#                      T12=T12s, MAXT=MAXTs,
#                      nrep=epoch_mle_reps, Ne=epoch_Nes, U=Us, k=ks, 
#                      num_iter=num_iters,
#                      locus=loci, tCutoff=epoch_tCutoffs)
#
#rule all_mles_epochs:
#  input:
#    mles_results
#
#rule mles_epochs:
#  input:
#    process_trees_pattern, #processed treees
#    tree_pattern.replace("{sim_output}", tree_outputs[1]) #locations
#  output:
#    mles_pattern
#  threads: 40
#  run:
#    # get processed trees
#    processed_trees = np.load(input[0], allow_pickle=True)
#    coal_times = processed_trees['coal_times']
#    pcoals = processed_trees['pcoals']
#    shared_times = processed_trees['shared_times']
#    samples = processed_trees['samples']
#    # get locations of samples
#    ind_locs = np.loadtxt(input[1]) #locations of diploid individuals
#    locations = np.repeat(ind_locs, 2, axis=0)[:,0:2] #chromosome locations in 2D
#    # get mle dispersal rate
#    x0 = np.array([float(wildcards.SIGMAd2), float(wildcards.SIGMAd2), 0, float(wildcards.SIGMAd1), float(wildcards.SIGMAd1), 0, 1]) #guesses for dispersal [sdx, sdy, corr] from most distant to most recent epoch
#    tsplits = [float(wildcards.T12)] #generations in past that want to let dispersal change stepwise
#    mle = sparg.find_mle(locations, coal_times, pcoals, shared_times, samples, x0=x0, bnds=epoch_bnds, tCutoff=int(wildcards.tCutoff), important=False, quiet=False, scale_phi=scale_phi, remove_missing=False, method=method, tsplits=tsplits)
#    if mle.success: 
#      np.save(output[0], mle)
#    else:
#      print('failed!')
#
## --------------------- MLEs of dispseral rate from inferred trees ----------------------
#
#inf_mles_pattern = process_inf_trees_pattern.replace('.npz', '_mle_dispersal.npy')
#
#inf_mles_results = expand(inf_mles_pattern,
#                          L=Ls, RBP=RBPs,
#                          LAMBDA=LAMBDAs, K=epoch_Ks,
#                          W=Ws, SIGMAi=SIGMAis,
#                          SIGMAd1=SIGMAd1s, SIGMAd2=SIGMAd2s,
#                          T12=T12s, MAXT=MAXTs,
#                          nrep=epoch_mle_reps, Ne=epoch_Nes, U=Us, k=ks, 
#                          num_iter=num_iters,
#                          locus=loci, tCutoff=epoch_tCutoffs, M=Ms)
#
#rule all_inf_mles_epochs:
#  input:
#    inf_mles_results
#
#rule inf_mles_epochs:
#  input:
#    process_inf_trees_pattern, #processed treees
#    reorder_locations_pattern #locations
#  output:
#    inf_mles_pattern
#  threads: 40
#  run:
#    # get processed trees
#    processed_trees = np.load(input[0], allow_pickle=True)
#    coal_times = processed_trees['coal_times']
#    pcoals = processed_trees['pcoals']
#    shared_times = processed_trees['shared_times']
#    samples = processed_trees['samples']
#    # get locations of samples
#    locations = np.loadtxt(input[1])[:,:2] #locations of haploid genomes
#    # get mle dispersal rate
#    x0 = np.array([float(wildcards.SIGMAd2), float(wildcards.SIGMAd2), 0, float(wildcards.SIGMAd1), float(wildcards.SIGMAd1), 0, 0.001]) #initial guess for dispersal SDs, correlation, and branching rate
#    tsplits = [float(wildcards.T12)]
#    mle = sparg.find_mle(locations, coal_times, pcoals, shared_times, samples, x0=x0, bnds=epoch_bnds, tCutoff=int(wildcards.tCutoff), important=True, quiet=False, scale_phi=scale_phi, remove_missing=False, method=method, tsplits=tsplits)
#    if not mle.success:
#      new_scale_phi = scale_phi * 10 #increase scale_phi by order of magnitude
#      print('search failed with scale_phi=%d, trying scale_phi=%d...' %(scale_phi,new_scale_phi))
#      #x0 = mle.x #take where we left off
#      x0[-1] = x0[-1] * new_scale_phi/scale_phi #rescale starting guess for phi
#      mle = sparg.find_mle(locations, coal_times, pcoals, shared_times, samples, x0=x0, bnds=epoch_bnds, tCutoff=int(wildcards.tCutoff), important=True, quiet=False, scale_phi=new_scale_phi, remove_missing=False, method=method, tsplits=tsplits)
#    if mle.success: 
#      np.save(output[0], mle)
#    else:
#      print('still failed!')
#
## --------------------- MCLEs of dispseral rate from true trees ----------------------
#
#mcles_pattern = mles_pattern.replace('mle', 'mcle').replace('_{locus}locus','')
#
#mcles_results = expand(mcles_pattern,
#                      L=Ls, RBP=RBPs,
#                      LAMBDA=LAMBDAs, K=epoch_Ks,
#                      W=Ws, SIGMAi=SIGMAis,
#                      SIGMAd1=SIGMAd1s, SIGMAd2=SIGMAd2s,
#                      T12=T12s, MAXT=MAXTs,
#                      nrep=epoch_reps, Ne=epoch_Nes, U=Us, k=ks, 
#                      num_iter=num_iters,
#                      tCutoff=epoch_tCutoffs)
#
#rule all_mcles_epochs:
#  input:
#    mcles_results
#
#rule mcles_epochs:
#  input:
#    expand(process_trees_pattern, locus=loci, allow_missing=True), #processed treees
#    tree_pattern.replace("{sim_output}", tree_outputs[1]) #locations
#  output:
#    mcles_pattern
#  threads: 40
#  run:
#    # get processed trees
#    coal_times = []
#    pcoals = []
#    shared_times = []
#    samples = []
#    for locus in loci:
#      processed_trees = np.load(input[locus], allow_pickle=True)
#      coal_times.append(processed_trees['coal_times'][0])
#      pcoals.append(processed_trees['pcoals'][0])
#      shared_times.append(processed_trees['shared_times'][0])
#      samples.append(processed_trees['samples'][0])
#    # get locations of samples
#    ind_locs = np.loadtxt(input[-1]) #locations of diploid individuals
#    locations = np.repeat(ind_locs, 2, axis=0)[:,0:2] #chromosome locations in 2D
#    # get mle dispersal rate
#    x0 = np.array([float(wildcards.SIGMAd2), float(wildcards.SIGMAd2), 0, float(wildcards.SIGMAd1), float(wildcards.SIGMAd1), 0, 1]) #guesses for dispersal [sdx, sdy, corr] from most distant to most recent epoch
#    tsplits = [float(wildcards.T12)] #generations in past that want to let dispersal change stepwise
#    mle = sparg.find_mle(locations, coal_times, pcoals, shared_times, samples, x0=x0, bnds=epoch_bnds, tCutoff=int(wildcards.tCutoff), important=False, quiet=False, scale_phi=scale_phi, remove_missing=False, method=method, tsplits=tsplits)
#    if mle.success: 
#      np.save(output[0], mle)
#    else:
#      print('failed!')
#
## --------------------- MCLEs of dispseral rate from inferred trees ----------------------
#
#inf_mcles_pattern = inf_mles_pattern.replace('mle', 'mcle').replace('_{locus}locus','')
#
#inf_mcles_results = expand(inf_mcles_pattern,
#                          L=Ls, RBP=RBPs,
#                          LAMBDA=LAMBDAs, K=epoch_Ks,
#                          W=Ws, SIGMAi=SIGMAis,
#                          SIGMAd1=SIGMAd1s, SIGMAd2=SIGMAd2s,
#                          T12=T12s, MAXT=MAXTs,
#                          nrep=epoch_reps, Ne=epoch_Nes, U=Us, k=ks, 
#                          num_iter=num_iters,
#                          tCutoff=epoch_tCutoffs, M=Ms)
#
#rule all_inf_mcles_epochs:
#  input:
#    inf_mcles_results
#
#rule inf_mcles_epochs:
#  input:
#    expand(process_inf_trees_pattern, locus=loci, allow_missing=True),#processed treees
#    reorder_locations_pattern #locations
#  output:
#    inf_mcles_pattern
#  resources:
#    time = 60
#  threads: 40
#  run:
#    # get processed trees
#    coal_times = []
#    pcoals = []
#    shared_times = []
#    samples = []
#    for locus in loci:
#      processed_trees = np.load(input[locus], allow_pickle=True)
#      coal_times.append(processed_trees['coal_times'][0])
#      pcoals.append(processed_trees['pcoals'][0])
#      shared_times.append(processed_trees['shared_times'][0])
#      samples.append(processed_trees['samples'][0])
#    # get locations of samples
#    locations = np.loadtxt(input[-1])[:,:2] #locations of haploid genomes
#    # get mle dispersal rate
#    x0 = np.array([float(wildcards.SIGMAd2), float(wildcards.SIGMAd2), 0, float(wildcards.SIGMAd1), float(wildcards.SIGMAd1), 0, 0.001]) #initial guess for dispersal SDs, correlation, and branching rate
#    tsplits = [float(wildcards.T12)]
#    mle = sparg.find_mle(locations, coal_times, pcoals, shared_times, samples, x0=x0, bnds=epoch_bnds, tCutoff=int(wildcards.tCutoff), important=True, quiet=False, scale_phi=scale_phi, remove_missing=False, method=method, tsplits=tsplits)
#    if not mle.success:
#      new_scale_phi = scale_phi * 10 #increase scale_phi by order of magnitude
#      print('search failed with scale_phi=%d, trying scale_phi=%d...' %(scale_phi,new_scale_phi))
#      #x0 = mle.x #take where we left off
#      x0[-1] = x0[-1] * new_scale_phi/scale_phi #rescale starting guess for phi
#      mle = sparg.find_mle(locations, coal_times, pcoals, shared_times, samples, x0=x0, bnds=epoch_bnds, tCutoff=int(wildcards.tCutoff), important=True, quiet=False, scale_phi=new_scale_phi, remove_missing=False, method=method, tsplits=tsplits)
#    if mle.success: 
#      np.save(output[0], mle)
#    else:
#      print('still failed!')
#
## --------------------- MCLEs of dispseral rate from true trees, 1 epoch ----------------------
#
#mcles_pattern = mcles_pattern.replace('.npy', '_1epoch.npy')
#
#mcles_results = expand(mcles_pattern,
#                      L=Ls, RBP=RBPs,
#                      LAMBDA=LAMBDAs, K=epoch_Ks,
#                      W=Ws, SIGMAi=SIGMAis,
#                      SIGMAd1=SIGMAd1s, SIGMAd2=SIGMAd2s,
#                      T12=T12s, MAXT=MAXTs,
#                      nrep=epoch_reps, Ne=epoch_Nes, U=Us, k=ks, 
#                      num_iter=num_iters,
#                      tCutoff=epoch_tCutoffs)
#
#rule all_mcles_epochs_one:
#  input:
#    mcles_results
#
#rule mcles_epochs_one:
#  input:
#    expand(process_trees_pattern, locus=loci, allow_missing=True), #processed treees
#    tree_pattern.replace("{sim_output}", tree_outputs[1]) #locations
#  output:
#    mcles_pattern
#  threads: 40
#  run:
#    # get processed trees
#    coal_times = []
#    pcoals = []
#    shared_times = []
#    samples = []
#    for locus in loci:
#      processed_trees = np.load(input[locus], allow_pickle=True)
#      coal_times.append(processed_trees['coal_times'][0])
#      pcoals.append(processed_trees['pcoals'][0])
#      shared_times.append(processed_trees['shared_times'][0])
#      samples.append(processed_trees['samples'][0])
#    # get locations of samples
#    ind_locs = np.loadtxt(input[-1]) #locations of diploid individuals
#    locations = np.repeat(ind_locs, 2, axis=0)[:,0:2] #chromosome locations in 2D
#    # get mle dispersal rate
#    x0 = np.array([(float(wildcards.SIGMAd2) + float(wildcards.SIGMAd1))/2, (float(wildcards.SIGMAd2) + float(wildcards.SIGMAd1))/2, 0, 1]) #guesses for dispersal [sdx, sdy, corr] from most distant to most recent epoch
#    mle = sparg.find_mle(locations, coal_times, pcoals, shared_times, samples, x0=x0, bnds=bnds, tCutoff=int(wildcards.tCutoff), important=False, quiet=False, scale_phi=scale_phi, remove_missing=False, method=method)
#    if mle.success: 
#      np.save(output[0], mle)
#    else:
#      print('failed!')
#
## --------------------- MCLEs of dispseral rate from inferred trees ----------------------
#
#inf_mcles_pattern = inf_mcles_pattern.replace('.npy', '_1epoch.npy')
#
#inf_mcles_results = expand(inf_mcles_pattern,
#                          L=Ls, RBP=RBPs,
#                          LAMBDA=LAMBDAs, K=epoch_Ks,
#                          W=Ws, SIGMAi=SIGMAis,
#                          SIGMAd1=SIGMAd1s, SIGMAd2=SIGMAd2s,
#                          T12=T12s, MAXT=MAXTs,
#                          nrep=epoch_reps, Ne=epoch_Nes, U=Us, k=ks, 
#                          num_iter=num_iters,
#                          tCutoff=epoch_tCutoffs, M=Ms)
#
#rule all_inf_mcles_epochs_one:
#  input:
#    inf_mcles_results
#
#rule inf_mcles_epochs_one:
#  input:
#    expand(process_inf_trees_pattern, locus=loci, allow_missing=True),#processed treees
#    reorder_locations_pattern #locations
#  output:
#    inf_mcles_pattern
#  resources:
#    time = 60
#  threads: 40
#  run:
#    # get processed trees
#    coal_times = []
#    pcoals = []
#    shared_times = []
#    samples = []
#    for locus in loci:
#      processed_trees = np.load(input[locus], allow_pickle=True)
#      coal_times.append(processed_trees['coal_times'][0])
#      pcoals.append(processed_trees['pcoals'][0])
#      shared_times.append(processed_trees['shared_times'][0])
#      samples.append(processed_trees['samples'][0])
#    # get locations of samples
#    locations = np.loadtxt(input[-1])[:,:2] #locations of haploid genomes
#    # get mle dispersal rate
#    x0 = np.array([(float(wildcards.SIGMAd2) + float(wildcards.SIGMAd1))/2, (float(wildcards.SIGMAd2) + float(wildcards.SIGMAd1))/2, 0, 1]) #guesses for dispersal [sdx, sdy, corr] from most distant to most recent epoch
#    mle = sparg.find_mle(locations, coal_times, pcoals, shared_times, samples, x0=x0, bnds=bnds, tCutoff=int(wildcards.tCutoff), important=True, quiet=False, scale_phi=scale_phi, remove_missing=False, method=method)
#    if not mle.success:
#      new_scale_phi = scale_phi * 10 #increase scale_phi by order of magnitude
#      print('search failed with scale_phi=%d, trying scale_phi=%d...' %(scale_phi,new_scale_phi))
#      #x0 = mle.x #take where we left off
#      x0[-1] = x0[-1] * new_scale_phi/scale_phi #rescale starting guess for phi
#      mle = sparg.find_mle(locations, coal_times, pcoals, shared_times, samples, x0=x0, bnds=bnds, tCutoff=int(wildcards.tCutoff), important=True, quiet=False, scale_phi=new_scale_phi, remove_missing=False, method=method)
#    if mle.success: 
#      np.save(output[0], mle)
#    else:
#      print('still failed!')
